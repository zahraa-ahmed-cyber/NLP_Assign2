{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "NLP_assign2",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "7sxdCfHDx8a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Install and Import Dependencies"
      ],
      "metadata": {
        "id": "3G8sxMxqx8a7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:02:20.0107Z",
          "iopub.execute_input": "2024-03-25T08:02:20.010964Z",
          "iopub.status.idle": "2024-03-25T08:02:32.733476Z",
          "shell.execute_reply.started": "2024-03-25T08:02:20.010943Z",
          "shell.execute_reply": "2024-03-25T08:02:32.732551Z"
        },
        "trusted": true,
        "id": "YSY5Kgyjx8bB",
        "outputId": "6b957a1e-497e-4982-a49a-d222d38b1966"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:02:32.735457Z",
          "iopub.execute_input": "2024-03-25T08:02:32.735749Z",
          "iopub.status.idle": "2024-03-25T08:02:50.893149Z",
          "shell.execute_reply.started": "2024-03-25T08:02:32.735718Z",
          "shell.execute_reply": "2024-03-25T08:02:50.892291Z"
        },
        "trusted": true,
        "id": "O8RctT0ix8bD",
        "outputId": "2be6103c-7720-4b93-b7fb-855033457f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-03-25 08:02:34.456275: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-25 08:02:34.456407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-25 08:02:34.595800: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:02:50.898361Z",
          "iopub.execute_input": "2024-03-25T08:02:50.898671Z",
          "iopub.status.idle": "2024-03-25T08:02:56.170886Z",
          "shell.execute_reply.started": "2024-03-25T08:02:50.898641Z",
          "shell.execute_reply": "2024-03-25T08:02:56.169947Z"
        },
        "trusted": true,
        "id": "uHQ0uKidx8bE",
        "outputId": "8208ef16-0fae-4edb-9cce-613b0040077b",
        "colab": {
          "referenced_widgets": [
            "39fdfd9651b04d7fbf9607cf35a0c0a6",
            "291beffd3dec44769cd15045d336a867",
            "0fb3730500244498966a57ea32ff5333",
            "660724e86a6243398a9004209428a6ee",
            "acbb4df465ae4df0b9d696ed5cdfdc57"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39fdfd9651b04d7fbf9607cf35a0c0a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "291beffd3dec44769cd15045d336a867"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fb3730500244498966a57ea32ff5333"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "660724e86a6243398a9004209428a6ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acbb4df465ae4df0b9d696ed5cdfdc57"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:02:56.172036Z",
          "iopub.execute_input": "2024-03-25T08:02:56.172349Z",
          "iopub.status.idle": "2024-03-25T08:05:48.287833Z",
          "shell.execute_reply.started": "2024-03-25T08:02:56.172323Z",
          "shell.execute_reply": "2024-03-25T08:05:48.2871Z"
        },
        "trusted": true,
        "id": "3Hy-0Nvqx8bF",
        "outputId": "ce2edd9e-94f2-4926-a7f3-6308ae8b651e",
        "colab": {
          "referenced_widgets": [
            "e1d8e555621545dda5fef0cab09018dc",
            "c4b5e86490b04406bb5ae8be2658f1b9"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1d8e555621545dda5fef0cab09018dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4b5e86490b04406bb5ae8be2658f1b9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'I want to learn data analysis'\n",
        "input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
        "input_ids\n",
        "# generate text until the output length (which includes the context length) reaches 50\n",
        "output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "output\n",
        "doc1 = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "doc1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:05:48.289131Z",
          "iopub.execute_input": "2024-03-25T08:05:48.2895Z",
          "iopub.status.idle": "2024-03-25T08:06:22.180906Z",
          "shell.execute_reply.started": "2024-03-25T08:05:48.289469Z",
          "shell.execute_reply": "2024-03-25T08:06:22.179868Z"
        },
        "trusted": true,
        "id": "lbnl75wsx8bG",
        "outputId": "f13c73d3-4726-455e-e42c-d1ac9f55aba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "\"I want to learn data analysis, but I don't know where to start.\\n\\nI'm not sure what to do with all the data I've collected. I'm trying to figure out how to use it to make better decisions about my life. How can I use this data to improve my health? How do I make the most of my free time? What's the best way to spend my time and money?\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Machine learning'\n",
        "input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
        "input_ids\n",
        "# generate text until the output length (which includes the context length) reaches 50\n",
        "output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "output\n",
        "doc2 = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "doc2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:06:22.182413Z",
          "iopub.execute_input": "2024-03-25T08:06:22.182697Z",
          "iopub.status.idle": "2024-03-25T08:07:03.338297Z",
          "shell.execute_reply.started": "2024-03-25T08:06:22.182673Z",
          "shell.execute_reply": "2024-03-25T08:07:03.337309Z"
        },
        "trusted": true,
        "id": "XqlvF8Rwx8bH",
        "outputId": "a3d2a735-95c0-4281-831f-6df9d8a6a352"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "\"Machine learning is the process of using machine learning techniques to predict the future behavior of a system.\\n\\nIn this tutorial, you will learn how to use the Python programming language to create a simple neural network. You will also learn about the different types of neural networks and how they can be used to make predictions. This tutorial is intended to be a beginner's introduction to the world of deep learning. If you are already familiar with Python, then you can skip this section and continue with the rest of\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'I like eating sushi'\n",
        "input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
        "input_ids\n",
        "# generate text until the output length (which includes the context length) reaches 50\n",
        "output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "output\n",
        "doc3 = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "doc3"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:07:03.339583Z",
          "iopub.execute_input": "2024-03-25T08:07:03.340113Z",
          "iopub.status.idle": "2024-03-25T08:07:38.21506Z",
          "shell.execute_reply.started": "2024-03-25T08:07:03.340088Z",
          "shell.execute_reply": "2024-03-25T08:07:38.214117Z"
        },
        "trusted": true,
        "id": "WvGwC-bUx8bI",
        "outputId": "1035ec3c-e732-46ed-c63f-e5a668042062"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'I like eating sushi, but I don\\'t like sushi that much,\" he said. \"It\\'s just not my thing.\"\\n\\n\"I\\'m not a big fan of sushi. I think it\\'s a little bit of a waste of food. It doesn\\'t taste very good. And I\\'m a vegetarian, so that\\'s not an issue for me. But if I had to choose, I\\'d probably choose sushi.\"'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = doc1 + \" \" + doc2 + \" \" + doc3\n",
        "documents"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:07:38.2164Z",
          "iopub.execute_input": "2024-03-25T08:07:38.216829Z",
          "iopub.status.idle": "2024-03-25T08:07:38.223367Z",
          "shell.execute_reply.started": "2024-03-25T08:07:38.216796Z",
          "shell.execute_reply": "2024-03-25T08:07:38.222489Z"
        },
        "trusted": true,
        "id": "ePOy6V5_x8bI",
        "outputId": "dd295447-fe9b-45f2-f202-44d8c43dc008"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'I want to learn data analysis, but I don\\'t know where to start.\\n\\nI\\'m not sure what to do with all the data I\\'ve collected. I\\'m trying to figure out how to use it to make better decisions about my life. How can I use this data to improve my health? How do I make the most of my free time? What\\'s the best way to spend my time and money? Machine learning is the process of using machine learning techniques to predict the future behavior of a system.\\n\\nIn this tutorial, you will learn how to use the Python programming language to create a simple neural network. You will also learn about the different types of neural networks and how they can be used to make predictions. This tutorial is intended to be a beginner\\'s introduction to the world of deep learning. If you are already familiar with Python, then you can skip this section and continue with the rest of I like eating sushi, but I don\\'t like sushi that much,\" he said. \"It\\'s just not my thing.\"\\n\\n\"I\\'m not a big fan of sushi. I think it\\'s a little bit of a waste of food. It doesn\\'t taste very good. And I\\'m a vegetarian, so that\\'s not an issue for me. But if I had to choose, I\\'d probably choose sushi.\"'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:07:38.227356Z",
          "iopub.execute_input": "2024-03-25T08:07:38.227956Z",
          "iopub.status.idle": "2024-03-25T08:07:39.587646Z",
          "shell.execute_reply.started": "2024-03-25T08:07:38.22792Z",
          "shell.execute_reply": "2024-03-25T08:07:39.58673Z"
        },
        "trusted": true,
        "id": "z-CqtcxFx8bJ",
        "outputId": "d41d540a-08c4-4b40-9431-d41e9692c40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import unicodedata\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_and_tokenize(text):\n",
        "\n",
        "    # Normalize text (convert to lowercase and remove accents)\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8').lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    text = text.replace('\\n', '')\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return tokens , text\n",
        "\n",
        "\n",
        "\n",
        "documents_tokens , text = preprocess_and_tokenize(documents)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:07:39.589017Z",
          "iopub.execute_input": "2024-03-25T08:07:39.589326Z",
          "iopub.status.idle": "2024-03-25T08:07:43.699232Z",
          "shell.execute_reply.started": "2024-03-25T08:07:39.589278Z",
          "shell.execute_reply": "2024-03-25T08:07:43.698447Z"
        },
        "trusted": true,
        "id": "EhPxuKJlx8bK",
        "outputId": "5fe9b564-699b-4349-9189-0f56170331f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:07:43.70056Z",
          "iopub.execute_input": "2024-03-25T08:07:43.70136Z",
          "iopub.status.idle": "2024-03-25T08:07:43.707913Z",
          "shell.execute_reply.started": "2024-03-25T08:07:43.701321Z",
          "shell.execute_reply": "2024-03-25T08:07:43.707017Z"
        },
        "trusted": true,
        "id": "vgY9msGjx8bK",
        "outputId": "576f09cc-97ba-416d-ec83-8ed8e56a825c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'i want to learn data analysis but i dont know where to startim not sure what to do with all the data ive collected im trying to figure out how to use it to make better decisions about my life how can i use this data to improve my health how do i make the most of my free time whats the best way to spend my time and money machine learning is the process of using machine learning techniques to predict the future behavior of a systemin this tutorial you will learn how to use the python programming language to create a simple neural network you will also learn about the different types of neural networks and how they can be used to make predictions this tutorial is intended to be a beginners introduction to the world of deep learning if you are already familiar with python then you can skip this section and continue with the rest of i like eating sushi but i dont like sushi that much he said its just not my thingim not a big fan of sushi i think its a little bit of a waste of food it doesnt taste very good and im a vegetarian so thats not an issue for me but if i had to choose id probably choose sushi'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "unique_documents = np.unique(documents_tokens)\n",
        "unique_documents"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:07:43.709027Z",
          "iopub.execute_input": "2024-03-25T08:07:43.709347Z",
          "iopub.status.idle": "2024-03-25T08:07:43.71983Z",
          "shell.execute_reply.started": "2024-03-25T08:07:43.709321Z",
          "shell.execute_reply": "2024-03-25T08:07:43.718915Z"
        },
        "trusted": true,
        "id": "2tYX3GgUx8bL",
        "outputId": "2a00dd4c-5783-4bc5-9d0b-720b58aefa9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array(['already', 'also', 'analysis', 'beginner', 'behavior', 'best',\n       'better', 'big', 'bit', 'choose', 'collected', 'continue',\n       'create', 'data', 'decision', 'deep', 'different', 'doesnt',\n       'dont', 'eating', 'familiar', 'fan', 'figure', 'food', 'free',\n       'future', 'good', 'health', 'id', 'im', 'improve', 'intended',\n       'introduction', 'issue', 'ive', 'know', 'language', 'learn',\n       'learning', 'life', 'like', 'little', 'machine', 'make', 'money',\n       'much', 'network', 'neural', 'predict', 'prediction', 'probably',\n       'process', 'programming', 'python', 'rest', 'said', 'section',\n       'simple', 'skip', 'spend', 'startim', 'sure', 'sushi', 'systemin',\n       'taste', 'technique', 'thats', 'thingim', 'think', 'time',\n       'trying', 'tutorial', 'type', 'use', 'used', 'using', 'vegetarian',\n       'want', 'waste', 'way', 'whats', 'world'], dtype='<U12')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(unique_documents)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:07:43.720948Z",
          "iopub.execute_input": "2024-03-25T08:07:43.721237Z",
          "iopub.status.idle": "2024-03-25T08:07:43.731712Z",
          "shell.execute_reply.started": "2024-03-25T08:07:43.721213Z",
          "shell.execute_reply": "2024-03-25T08:07:43.730904Z"
        },
        "trusted": true,
        "id": "IpLM7S50x8bL",
        "outputId": "0b89df2c-4e71-4ed0-9757-f5cc4b11b10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "82"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [doc1 , doc2 , doc3]\n",
        "docs\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:07:43.732986Z",
          "iopub.execute_input": "2024-03-25T08:07:43.733267Z",
          "iopub.status.idle": "2024-03-25T08:07:43.743287Z",
          "shell.execute_reply.started": "2024-03-25T08:07:43.733227Z",
          "shell.execute_reply": "2024-03-25T08:07:43.742396Z"
        },
        "trusted": true,
        "id": "ejqQQlF6x8bL",
        "outputId": "3693db4e-8208-423a-9ed1-c68084672497"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[\"I want to learn data analysis, but I don't know where to start.\\n\\nI'm not sure what to do with all the data I've collected. I'm trying to figure out how to use it to make better decisions about my life. How can I use this data to improve my health? How do I make the most of my free time? What's the best way to spend my time and money?\",\n \"Machine learning is the process of using machine learning techniques to predict the future behavior of a system.\\n\\nIn this tutorial, you will learn how to use the Python programming language to create a simple neural network. You will also learn about the different types of neural networks and how they can be used to make predictions. This tutorial is intended to be a beginner's introduction to the world of deep learning. If you are already familiar with Python, then you can skip this section and continue with the rest of\",\n 'I like eating sushi, but I don\\'t like sushi that much,\" he said. \"It\\'s just not my thing.\"\\n\\n\"I\\'m not a big fan of sushi. I think it\\'s a little bit of a waste of food. It doesn\\'t taste very good. And I\\'m a vegetarian, so that\\'s not an issue for me. But if I had to choose, I\\'d probably choose sushi.\"']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Compute TF-IDF using sklearn\n",
        "vectorizer = TfidfVectorizer(vocabulary=unique_documents)\n",
        "tfidf_matrix = vectorizer.fit_transform(docs)\n",
        "\n",
        "# Print feature names\n",
        "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print TF-IDF matrix\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:07:43.744407Z",
          "iopub.execute_input": "2024-03-25T08:07:43.744702Z",
          "iopub.status.idle": "2024-03-25T08:07:43.771067Z",
          "shell.execute_reply.started": "2024-03-25T08:07:43.744677Z",
          "shell.execute_reply": "2024-03-25T08:07:43.770223Z"
        },
        "trusted": true,
        "id": "687Vkg5wx8bM",
        "outputId": "4a6aec5c-7f31-4b27-81d4-b5e68af53dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Feature names: ['already' 'also' 'analysis' 'beginner' 'behavior' 'best' 'better' 'big'\n 'bit' 'choose' 'collected' 'continue' 'create' 'data' 'decision' 'deep'\n 'different' 'doesnt' 'dont' 'eating' 'familiar' 'fan' 'figure' 'food'\n 'free' 'future' 'good' 'health' 'id' 'im' 'improve' 'intended'\n 'introduction' 'issue' 'ive' 'know' 'language' 'learn' 'learning' 'life'\n 'like' 'little' 'machine' 'make' 'money' 'much' 'network' 'neural'\n 'predict' 'prediction' 'probably' 'process' 'programming' 'python' 'rest'\n 'said' 'section' 'simple' 'skip' 'spend' 'startim' 'sure' 'sushi'\n 'systemin' 'taste' 'technique' 'thats' 'thingim' 'think' 'time' 'trying'\n 'tutorial' 'type' 'use' 'used' 'using' 'vegetarian' 'want' 'waste' 'way'\n 'whats' 'world']\nTF-IDF Matrix:\n[[0.         0.         0.17098241 0.         0.         0.17098241\n  0.17098241 0.         0.         0.         0.17098241 0.\n  0.         0.51294722 0.         0.         0.         0.\n  0.         0.         0.         0.         0.17098241 0.\n  0.17098241 0.         0.         0.17098241 0.         0.\n  0.17098241 0.         0.         0.         0.         0.17098241\n  0.         0.13003653 0.         0.17098241 0.         0.\n  0.         0.26007305 0.17098241 0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.17098241\n  0.         0.17098241 0.         0.         0.         0.\n  0.         0.         0.         0.34196481 0.17098241 0.\n  0.         0.26007305 0.         0.         0.         0.17098241\n  0.         0.17098241 0.         0.        ]\n [0.13805204 0.13805204 0.         0.13805204 0.13805204 0.\n  0.         0.         0.         0.         0.         0.13805204\n  0.13805204 0.         0.         0.13805204 0.13805204 0.\n  0.         0.         0.13805204 0.         0.         0.\n  0.         0.13805204 0.         0.         0.         0.\n  0.         0.13805204 0.13805204 0.         0.         0.\n  0.13805204 0.20998427 0.41415612 0.         0.         0.\n  0.27610408 0.10499213 0.         0.         0.13805204 0.27610408\n  0.13805204 0.         0.         0.13805204 0.13805204 0.27610408\n  0.13805204 0.         0.13805204 0.13805204 0.13805204 0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.27610408\n  0.         0.10499213 0.13805204 0.13805204 0.         0.\n  0.         0.         0.         0.13805204]\n [0.         0.         0.         0.         0.         0.\n  0.         0.16012815 0.16012815 0.32025631 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.16012815 0.         0.16012815 0.         0.16012815\n  0.         0.         0.16012815 0.         0.         0.\n  0.         0.         0.         0.16012815 0.         0.\n  0.         0.         0.         0.         0.32025631 0.16012815\n  0.         0.         0.         0.16012815 0.         0.\n  0.         0.         0.16012815 0.         0.         0.\n  0.         0.16012815 0.         0.         0.         0.\n  0.         0.         0.64051262 0.         0.16012815 0.\n  0.         0.         0.16012815 0.         0.         0.\n  0.         0.         0.         0.         0.16012815 0.\n  0.16012815 0.         0.         0.        ]]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:07:43.772173Z",
          "iopub.execute_input": "2024-03-25T08:07:43.772508Z",
          "iopub.status.idle": "2024-03-25T08:07:43.782264Z",
          "shell.execute_reply.started": "2024-03-25T08:07:43.772477Z",
          "shell.execute_reply": "2024-03-25T08:07:43.781461Z"
        },
        "trusted": true,
        "id": "ImrGYmQUx8bM",
        "outputId": "49c6d482-e0ea-4ae8-92cb-8ffb0e141f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(3, 82)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def calculate_tf(document, unique_words):\n",
        "    words_count = len(document.split())\n",
        "    word_freq = {}\n",
        "    for word in document.split():\n",
        "        word_freq[word] = word_freq.get(word, 0) + 1/ words_count\n",
        "    tf_vector = [word_freq[word] if word in word_freq else 0 for word in unique_words]\n",
        "    return tf_vector\n",
        "\n",
        "def calculate_idf(documents, unique_words):\n",
        "    idf_values = {}\n",
        "    total_docs = len(documents)\n",
        "    for word in unique_words:\n",
        "        word_count = sum(1 for document in documents if word in document)\n",
        "        idf_values[word] = math.log( 1 + total_docs / (1 + word_count), math.e) + 1  # Changed the equation\n",
        "    return idf_values\n",
        "\n",
        "def calculate_tfidf_matrix(documents, unique_words):\n",
        "    tfidf_matrix = []\n",
        "    idf_values = calculate_idf(documents, unique_words)\n",
        "    for document in documents:\n",
        "        tf_vector = calculate_tf(document, unique_words)\n",
        "        tfidf_vector = [tf_value * idf_values[word] for tf_value, word in zip(tf_vector, unique_words)]\n",
        "        tfidf_matrix.append(tfidf_vector)\n",
        "    tfidf_matrix = np.array(tfidf_matrix)\n",
        "    norms = np.sqrt(np.sum(tfidf_matrix**2, axis=1))  # Calculate the square root of the sum of squares\n",
        "    normalized_tfidf_matrix = tfidf_matrix / norms[:, np.newaxis]  # Normalize each row by dividing by its norm\n",
        "    return normalized_tfidf_matrix\n",
        "\n",
        "tfidf_matrix = calculate_tfidf_matrix(docs, unique_documents)\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix)\n",
        "print(\"Shape:\", tfidf_matrix.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T08:07:43.793421Z",
          "iopub.execute_input": "2024-03-25T08:07:43.79368Z",
          "iopub.status.idle": "2024-03-25T08:07:43.811075Z",
          "shell.execute_reply.started": "2024-03-25T08:07:43.793659Z",
          "shell.execute_reply": "2024-03-25T08:07:43.810221Z"
        },
        "trusted": true,
        "id": "Ei_CrQBxx8bM",
        "outputId": "a2402fa4-c4da-4901-cdaf-2f882249f54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "TF-IDF Matrix:\n[[0.         0.         0.         0.         0.         0.19781166\n  0.19781166 0.         0.         0.         0.         0.\n  0.         0.59343499 0.         0.         0.         0.\n  0.         0.         0.         0.         0.19781166 0.\n  0.19781166 0.         0.         0.         0.         0.\n  0.19781166 0.         0.         0.         0.         0.19781166\n  0.         0.14074463 0.         0.         0.         0.\n  0.         0.28148926 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.19781166\n  0.         0.19781166 0.         0.         0.         0.\n  0.         0.         0.         0.19781166 0.19781166 0.\n  0.         0.28148926 0.         0.         0.         0.19781166\n  0.         0.19781166 0.         0.        ]\n [0.16894046 0.16894046 0.         0.         0.16894046 0.\n  0.         0.         0.         0.         0.         0.16894046\n  0.16894046 0.         0.         0.16894046 0.16894046 0.\n  0.         0.         0.16894046 0.         0.         0.\n  0.         0.16894046 0.         0.         0.         0.\n  0.         0.16894046 0.16894046 0.         0.         0.\n  0.16894046 0.24040506 0.33788092 0.         0.         0.\n  0.16894046 0.12020253 0.         0.         0.         0.33788092\n  0.16894046 0.         0.         0.16894046 0.16894046 0.\n  0.16894046 0.         0.16894046 0.16894046 0.16894046 0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.16894046\n  0.         0.12020253 0.16894046 0.16894046 0.         0.\n  0.         0.         0.         0.16894046]\n [0.         0.         0.         0.         0.         0.\n  0.         0.25       0.25       0.25       0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.25       0.         0.25       0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.25       0.         0.\n  0.         0.         0.         0.         0.5        0.25\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.25       0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.25       0.         0.25       0.\n  0.         0.         0.25       0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.25       0.         0.         0.        ]]\nShape: (3, 82)\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}